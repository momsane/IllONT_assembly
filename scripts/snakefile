#command to symlink scratch: ln -s /scratch/mgarci14 ../../results/scratch
#wd=/work/FAC/FBM/DMF/pengel/general_data/syncom_ONT_seq/workflow/scripts
#workflow=scripts, envs, config
#data
#results
#logs

config_file = "../config/metadata.tsv"
# Contains one column for sample, one column with 1/0 to report if all the data is available,
# one column with the type of adapter to trim

# Import packages
import numpy as np
import pandas as pd

#Read config file
metadata = pd.read_table(config_file, sep='\t', header=0).set_index('sample', drop = False)

#List samples
SAMPLES = list(metadata.loc['avail' == 1, 'sample'])

# Put here the paths to the raw reads on the cluster
ILLUMINA_RAW = "../../data/raw_Illumina_reads"
ONT_RAW = "../../data/raw_ONT_reads"

rule all:
    input:
        "../../results/check_integrity/corrupted_files.txt",
        "../../results/count_reads_bt/count_reads_bt.txt",
        "../../results/fastqc_pretrim/all"

# The pipeline assumes the reads from different lanes are already
# concatenated into a single file

############################ ILLUMINA ##########################################

# File names should be something like: {sample}_R1.fastq.gz

# First we QC the Illumina reads

## Check that all raw read files are complete.

rule gzip_test:
    input:
        R1 = ILLUMINA_RAW + "/{sample}_R1.fastq.gz",
        R2 = ILLUMINA_RAW + "/{sample}_R2.fastq.gz",
    output:
        "../../results/check_integrity/samples/{sample}.txt"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "20m",
        mem_mb = 500
    shell:
        """
        if gzip -t {input.R1}; then
            echo ""
        else 
            echo "$file" >> {output}
        fi
        if gzip -t {input.R2}; then
            echo ""
        else 
            echo "$file" >> {output}
        fi
        """

# Incomplete/empty files are now printed to one file.
# ! Check if the output of the following rule is empty!

rule check_integrity:
    input:
        expand("../../results/check_integrity/samples/{sample}.txt", sample = SAMPLES)
    output:
        "../../results/check_integrity/corrupted_files.txt"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "10m",
        mem_mb = 100
    shell:
        """
        echo {input} | xargs cat >> {output}
        """

# Count number of reads in each fastq file before trimming.

rule count_reads_bt:
    input:
        R1 = ILLUMINA_RAW + "/{sample}_R1.fastq.gz",
        R2 = ILLUMINA_RAW + "/{sample}_R2.fastq.gz",
    output:
        "../../results/count_reads_bt/samples/count_reads_bt_{sample}.txt"
    params:
        tmp = "../../results/count_reads_bt/samples/count_reads_bt_{sample}.temp.txt"
    conda:
        "../envs/seqkit-2.6.1.yaml"
    log:
        "../../logs/count_reads_bt/samples/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "1h",
        mem_mb = 2000
    shell:
        """ 
        seqkit stats -T -b -e {input.R1} > {params.tmp};
        seqkit stats -T -b -e {input.R2} >> {params.tmp};
        cat {params.tmp} | awk -v sample={wildcards.sample} 'NR == 2 {{print sample"\tR1"$0}}' > {output};
        cat {params.tmp} | awk -v sample={wildcards.sample} 'NR == 2 {{print sample"\tR2"$0}}' >> {output};
        rm {params.tmp}
        """

# Combining all the samples stats into one file 

rule combine_readcounts_bt:
    input:
        expand("../../results/count_reads_bt/samples/count_reads_bt_{sample}.txt", sample = SAMPLES)
    output:
        "../../results/count_reads_bt/count_reads_bt.txt"
    log:
        "../../logs/count_reads_bt/combine_readcounts.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 1000
    shell:
        """
        echo -e "samples\tread\tfile\tformat\ttype\tnum_seqs\tsum_len\tmin_len\tavg_len\tmax_len" > {output};
        echo {input} | xargs cat >> {output}
        """

rule fastqc_bt:
    input:
        R1 = ILLUMINA_RAW + "/{sample}_R1.fastq.gz",
        R2 = ILLUMINA_RAW + "/{sample}_R2.fastq.gz",
    output:
        directory("../../results/fastqc_pretrim/samples/{sample}")
    conda:
        "../envs/fastqc-0.11.8.yaml"
    log:
        "../../logs/fastqc_pretrim/samples/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "1h",
        mem_mb = 1000
    shell:
        """
        mkdir -p {output};
        fastqc -o {output} {input.R1} {input.R2}
        """

# Compile and visualize all fastqc reports together

rule multiqc_fastqc_bt_bioproj:
    input:
        expand("../../results/fastqc_pretrim/samples/{sample}", sample = SAMPLES)
    output:
        directory("../../results/fastqc_pretrim/all")
    conda:
        "../envs/multiqc-1.6.yaml"
    params:
        ignore = ".zip"
    log:
        "../../logs/fastqc_pretrim/multiqc.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 2000
    shell:
        "multiqc --interactive -f --ignore {params.ignore} -o {output} {input}"

### TODO

# rule rawreads_trimming:
#     input:
#         R1=lambda wildcards: config["Illumina"][wildcards.sample][0],
#         R2=lambda wildcards: config["Illumina"][wildcards.sample][1]
#     output:
#         R1_paired="../data/Illumina_reads/{sample}_L1_R1_cut_trim.fastq.gz",
#         R2_paired="../data/Illumina_reads/{sample}_L1_R2_cut_trim.fastq.gz"
#     threads: 8
#     params:
#         nextera="../resources/short_refSeqs/NexteraPE-PE.fa",
#         q=26,
#         min_length=50,
#         R1_unpaired="../data/Illumina_reads/{sample}_R1_unpaired.fastq.gz",
#         R2_unpaired="../data/Illumina_reads/{sample}_R2_unpaired.fastq.gz"
#     log:
#         "logs/trimming/{sample}_trimming.log"
#     conda:
#         "envs/trimmomatic.yaml"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 6000,
#         runtime= "01:00:00"
#     shell:
#         "trimmomatic PE -phred33 -threads {threads} {input.R1} {input.R2} \
#          {output.R1_paired} {params.R1_unpaired} {output.R2_paired} {params.R2_unpaired} \
#          ILLUMINACLIP:{params.nextera}:4:25:8 \
#          SLIDINGWINDOW:5:{params.q} MINLEN:{params.min_length}"

# rule fastQC_PostTrimming:
#     input:
#         R1="../data/Illumina_reads/{sample}_L1_R1_cut_trim.fastq.gz",
#         R2="../data/Illumina_reads/{sample}_L1_R2_cut_trim.fastq.gz"
#     output:
#         directory("../results/QC/postTrimming/QC_{sample}/")
#     threads: 2
#     log:
#         "logs/Rawreads_QC/{sample}_postT_QC.log"
#     conda:
#         "envs/fastqc.yaml"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 6000,
#         runtime= "00:20:00"
#     shell:
#         "mkdir -p {output}; "
#         "fastqc -o {output} {input.R1} {input.R2}"


# ### From here on we use all the illumina Reads
# rule concat_lanes:
#     input:
#         R1s=get_direction_r1,
#         R2s=get_direction_r2
#     output:
#         R1_concat="../data/concat_illumina_reads/{sample}_R1_concat.fastq.gz",
#         R2_concat="../data/concat_illumina_reads/{sample}_R2_concat.fastq.gz"
#     threads: 2
#     log:
#         "logs/raw_reads_processing/lane_concatenation/{sample}_concat.log"
#     resources:
#         account = "pengel_beemicrophage",
#         runtime= "02:00:00"
#     shell:
#         "cat {input.R1s} > {output.R1_concat}; cat {input.R2s} > {output.R2_concat}"

# ############################ ONT ##########################################

# # Reads Processing
# rule ReadLength_stats:
#     input:
#         ONT=lambda wildcards: config["ONT"][wildcards.sample]
#     output:
#         "../results/raw_reads_processing/ONT/{sample}_raw_readLength.txt"
#     threads: 2
#     log:
#         "logs/raw_reads_processing/ONT/{sample}_raw_readLength.log"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 4000,
#         runtime= "00:30:00"
#     shell:
#         "echo '{wildcards.sample}' > {output}; "
#         "(less {input.ONT} | awk '{{if(NR%4==2) print length($1)}}' >> {output}) 2> {log} "

# rule ReadLength_summary:
#         input:
#             expand("../results/raw_reads_processing/ONT/{sample}_raw_readLength.txt", sample=config["samples"])
#         output:
#             "../results/raw_reads_processing/ONT/all_raw_readLength.txt"
#         threads: 2
#         log:
#             "logs/raw_reads_processing/ONT/summary_raw_readLength.log"
#         resources:
#             account = "pengel_beemicrophage",
#             mem_mb = 4000,
#             runtime= "00:30:00"
#         shell:
#             "(paste -d '\t' {input} > {output}) 2> {log}"

# rule read_filtering:
#     input:
#         lambda wildcards: config["ONT"][wildcards.sample]
#     output:
#         "../data/ONT_reads_filtered/{sample}_ONT_filtered.fastq"
#     threads: 5
#     log:
#         "logs/raw_reads_processing/ONT/{sample}_ONT_filtering.log"
#     conda:
#         "envs/filtlong.yaml"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 8000,
#         runtime= "02:00:00"
#     params:
#         MINIMUM_read_LENGTH=1000,
#         min_mean_q=10,
#         length_weight=10,
#         target_bases=400000000
#     shell:
#         "(filtlong --min_length {params.MINIMUM_read_LENGTH} \
#         --min_mean_q {params.min_mean_q} \
#         --length_weight {params.length_weight} \
#         --target_bases {params.target_bases}  \
#         {input}  > {output})2> {log}"

# rule ReadLength_AF_stats:
#     input:
#         ONT="../data/ONT_reads_filtered/{sample}_ONT_filtered.fastq"
#     output:
#         "../results/raw_reads_processing/ONT/after_filtering/{sample}_AF_readLength.txt"
#     threads: 2
#     log:
#         "logs/raw_reads_processing/ONT/{sample}_AF_readLength.log"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 4000,
#         runtime= "00:30:00"
#     shell:
#         "echo '{wildcards.sample}' > {output}; "
#         "(less {input.ONT} | awk '{{if(NR%4==2) print length($1)}}' >> {output}) 2> {log} "

# rule ReadLength_AF_summary:
#         input:
#             expand("../results/raw_reads_processing/ONT/after_filtering/{sample}_AF_readLength.txt", sample=config["samples"])
#         output:
#             "../results/raw_reads_processing/ONT/after_filtering/all_AF_readLength.txt"
#         threads: 2
#         log:
#             "logs/raw_reads_processing/ONT/summary_AF_readLength.log"
#         resources:
#             account = "pengel_beemicrophage",
#             mem_mb = 4000,
#             runtime= "00:30:00"
#         shell:
#             "(paste -d '\t' {input} > {output}) 2> {log}"

# # ONT assembly
# rule ONT_assembly:
#     input:
#         "../data/ONT_reads_filtered/{sample}_ONT_filtered.fastq"
#     output:
#         directory("../data/ONT_assembly/{sample}_ONT_assembly")
#     threads: 12
#     log:
#         "logs/ONT_assembly/{sample}_ONT_assembly.log"
#     conda:
#         "envs/flye.yaml"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 16000,
#         runtime= "03:00:00"
#     params:
#         #GENOME_SIZE=2.5m
#     shell:
#         "flye --threads {threads} --iterations 5 --nano-raw \
#       {input} --out-dir {output}"

# rule ONT_assembly_stats:
#     input:
#         dir=expand("../data/ONT_assembly/{sample}_ONT_assembly", sample=config["ONT"]),
#         stats=expand("../data/ONT_assembly/{sample}_ONT_assembly/30-contigger/contigs_stats.txt", sample=config["ONT"])
#     output:
#         "../data/ONT_assembly/all_contigs_stats.txt"
#     threads: 1
#     log:
#         "logs/ONT_assembly/summay_assembly.log"
#     params:
#         spacer="\t",
#         tmp="../data/ONT_assembly/tmp.txt"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 1000,
#         runtime= "00:20:00"
#     shell:
#         "echo -e 'seq_name\tlength\tcoverage\tcircular\trepeat\tmult\ttelomere\talt_group\tgraph_path\tsample' > {params.tmp}; "
#         "awk '{{print $0 {params.spacer} FILENAME; next;}}' {input.stats} >> {params.tmp}; "
#         "sed '/^#/ d' < {params.tmp} > {output}; "
#         "rm {params.tmp}"

# rule bandage_assembly:
#     input:
#         dir="../data/ONT_assembly/{sample}_ONT_assembly",
#         graph="../data/ONT_assembly/{sample}_ONT_assembly/assembly_graph.gfa"
#     output:
#         "../results/ONT_assembly/assembly_graphs/{sample}_ONT_assembly_graph.jpg"
#     threads: 2
#     log:
#         "logs/ONT_assembly/{sample}_ONT_assembly_graph.log"
#     conda:
#         "envs/bandage.yaml"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 2000,
#         runtime= "01:00:00"
#     shell:
#         "Bandage image {input.graph} {output}"

# rule ONT_polishing:
#     input:
#         ONT_reads="../data/ONT_reads_filtered/{sample}_ONT_filtered.fastq",
#         ONT_asmbl="../data/ONT_assembly/{sample}_ONT_assembly/assembly.fasta"
#     output:
#         dir=directory("../data/ONT_assembly/polishing/ONT/{sample}_polishing_ONT"),
#         asmbl1="../data/ONT_assembly/polishing/ONT/{sample}_polishing_ONT/01_first_Racon_graphmap/assembly/{sample}_first_Racon_Assembly.fasta",
#         asmbl2="../data/ONT_assembly/polishing/ONT/{sample}_polishing_ONT/02_second_Racon_graphmap/assembly/{sample}_second_Racon_Assembly.fasta"
#     threads: 12
#     log:
#         "logs/ONT_assembly/polishing/{sample}_ONT_polishing.log"
#     conda:
#         "envs/ONT_polishing.yaml"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 16000,
#         runtime= "03:00:00"
#     shell:
#         "./scripts/post_assembly/ONT_polishing.sh {wildcards.sample} {input.ONT_reads} {input.ONT_asmbl} \
#         {output.dir}"


# ############################ Both ##########################################
# rule RawReads_stats:
#     input:
#         ill="../data/concat_illumina_reads/{sample}_R1_concat.fastq.gz",
#         ONT=lambda wildcards: config["ONT"][wildcards.sample]
#     output:
#         "../results/raw_reads_processing/both/{sample}_raw_reads_stats.txt"
#     threads: 2
#     log:
#         "logs/raw_reads_processing/both/{sample}_read_count.log"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 4000,
#         runtime= "00:30:00"
#     shell:
#         "touch {output}; "
#         "(./scripts/pre_assembly/raw_reads_stats.sh {input.ill} {input.ONT} {output})2>{log}"

# rule illumina_polishing:
#     input:
#         ill_R1="../data/concat_illumina_reads/{sample}_R1_concat.fastq.gz",
#         ill_R2="../data/concat_illumina_reads/{sample}_R2_concat.fastq.gz",
#         dir="../data/ONT_assembly/polishing/ONT/{sample}_polishing_ONT",
#         ONT_asmbl="../data/ONT_assembly/polishing/ONT/{sample}_polishing_ONT/02_second_Racon_graphmap/assembly/{sample}_second_Racon_Assembly.fasta"
#     output:
#         dir=directory("../data/ONT_assembly/polishing/illumina/{sample}_polishing_illumina"),
#         file3="../data/ONT_assembly/polishing/illumina/{sample}_polishing_illumina/05_third_pillon_bowtie2/assembly/{sample}_third_Pilon_Assembly.fasta",
#         file2="../data/ONT_assembly/polishing/illumina/{sample}_polishing_illumina/04_second_pillon_bowtie2/assembly/{sample}_second_Pilon_Assembly.fasta",
#         file1="../data/ONT_assembly/polishing/illumina/{sample}_polishing_illumina/03_first_pillon_bowtie2/assembly/{sample}_first_Pilon_Assembly.fasta"
#     threads: 12
#     log:
#         "logs/ONT_assembly/polishing/illumina/{sample}_illumina_polishing.log"
#     # conda:
#     #     "envs/illumina_polishing.yaml" #TODO for some reason, if you run this in a conda env it doesn't work, skips the pilon step, so I'll use the cluster modules
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 16000,
#         runtime= "03:00:00"
#     shell:
#         "(./scripts/post_assembly/illumina_polishing.sh {threads} {input.ONT_asmbl} \
#         {input.ill_R1} {input.ill_R2} \
#         {output.dir} {wildcards.sample}) 2> {log}"

# rule assembly_QC:
#     input:
#         pillon3="../data/ONT_assembly/polishing/illumina/{sample}_polishing_illumina/05_third_pillon_bowtie2/assembly/{sample}_third_Pilon_Assembly.fasta",
#         ONT="../data/ONT_assembly/{sample}_ONT_assembly/assembly.fasta",
#         racon1="../data/ONT_assembly/polishing/ONT/{sample}_polishing_ONT/01_first_Racon_graphmap/assembly/{sample}_first_Racon_Assembly.fasta",
#         racon2="../data/ONT_assembly/polishing/ONT/{sample}_polishing_ONT/02_second_Racon_graphmap/assembly/{sample}_second_Racon_Assembly.fasta",
#         pillon1="../data/ONT_assembly/polishing/illumina/{sample}_polishing_illumina/03_first_pillon_bowtie2/assembly/{sample}_first_Pilon_Assembly.fasta",
#         pillon2="../data/ONT_assembly/polishing/illumina/{sample}_polishing_illumina/04_second_pillon_bowtie2/assembly/{sample}_second_Pilon_Assembly.fasta"
#     output:
#         directory("../results/ONT_assembly/polishing_QC/{sample}_assembly_QC/")
#     threads: 12
#     log:
#         "logs/ONT_assembly/polishing/quast/{sample}_assemblyQC.log"
#     conda:
#         "envs/quast.yaml"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 16000,
#         runtime= "03:00:00"
#     shell:
#         "(quast.py \
#         -l 'FlyeAssembly,first_ONT_racon,second_ONT_racon,first_Illumina_pilon,second_Illumina_pilon' \
#         -R {input.pillon3} \
#           {input.ONT} \
#           {input.racon1} \
#           {input.racon2} \
#           {input.pillon1} \
#           {input.pillon2} \
#           -o  {output}) 2> {log}"

# rule ONT_assembly_mapping:
#         input:
#             dir="../data/ONT_assembly/polishing/illumina/{sample}_polishing_illumina",
#             bam="../data/ONT_assembly/polishing/illumina/{sample}_polishing_illumina/04_second_pillon_bowtie2/bowtie2/second_pillon_ONT2FinalAssembly_sorted.bam"
#         output:
#             map_reads="../results/ONT_assembly/polishing_QC/{sample}_assembly_QC/{sample}_mapping.txt",
#             map_bases="../results/ONT_assembly/bases_cov/{sample}_read_mapping_per_base.txt"
#         threads: 12
#         log:
#             "logs/ONT_assembly/polishing/mapping/{sample}_mapping.log"
#         conda:
#             "envs/illumina_polishing.yaml"
#         resources:
#             account = "pengel_beemicrophage",
#             mem_mb = 16000,
#             runtime= "03:00:00"
#         shell:
#             "(samtools flagstat {input.bam} > {output.map_reads}) 2> {log}; "
#             "(samtools depth -a {input.bam} > {output.map_bases}) 2> {log}"

# rule Parse_ONT_assembly_mapping:
#     input:
#         dir=expand("../data/ONT_assembly/polishing/illumina/{sample}_polishing_illumina",sample=config["samples"]),
#         map_reads=expand("../results/ONT_assembly/polishing_QC/{sample}_assembly_QC/{sample}_mapping.txt", sample=config["samples"]),
#         map_bases=expand("../results/ONT_assembly/bases_cov/{sample}_read_mapping_per_base.txt", sample=config["samples"])
#     output:
#         all_reads="../results/ONT_assembly/polishing_QC/all_mapstats.txt",
#         all_bases="../results/ONT_assembly/bases_cov/all_bases_cov.txt"
#     log:
#         "logs/ONT_assembly/polishing/mapping/summary_mapping.log"
#     conda:
#         "envs/base_R_env.yaml"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 6000,
#         runtime= "00:30:00"
#     script:
#         "scripts/post_assembly/parse_ONTassembly.R"

# ################################################################################

# ############################ ANNOTATION ##########################################
# rule move_final_assemblies:
#     input:
#         asmbl=expand("../data/ONT_assembly/polishing/illumina/{sample}_polishing_illumina/05_third_pillon_bowtie2/assembly/{sample}_third_Pilon_Assembly.fasta", sample=config["samples"])
#     output:
#         db=directory("../resources/checkm_db"),
#         dir=directory("../results/ONT_assembly/final_assemblies/")
#     log:
#         "logs/annot_QC/move_assemblies/move_assembly.log"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 6000,
#         runtime= "00:30:00"
#     shell:
#         "mkdir -p {output.dir}; "
#         "cp {input.asmbl} {output.dir}/"

# rule checkm_QC:
#     input:
#         dir="../results/ONT_assembly/final_assemblies/",
#     output:
#         dir=directory("../results/ONT_assembly/checkm_QC/"),
#         file="../results/ONT_assembly/checkm_QC/checkm_QC_stats.txt"
#     log:
#         "logs/annot_QC/checkm/checkm_QC.log"
#     threads: 16
#     conda:
#         "envs/checkm_env.yaml"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 100000,
#         runtime= "01:00:00"
#     shell:
#         # "checkm_db={output.db}; "
#         # "echo ${{checkm_db}} | checkm data setRoot ${{checkm_db}}"
#         "export CHECKM_DATA_PATH=../resources/checkm_db"
#         "checkm lineage_wf {input.dir} {output.dir} -x .fasta -t {threads}; "
#         "checkm qa {output.dir}/lineage.ms {output.dir} -o 2 -f {output.file} --tab_table"

# rule checkm_plots:
#     input:
#         dir_asmbl="../results/ONT_assembly/final_assemblies/",
#         dir_checkm="../results/ONT_assembly/checkm_QC/"
#     output:
#         dir=directory("../results/ONT_assembly/checkm_QC/checkm_plots/")
#     log:
#         "logs/annot_QC/checkm/plot_checkm.log"
#     threads: 8
#     conda:
#         "envs/checkm_env.yaml"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 5000,
#         runtime= "01:00:00"
#     shell:
#         "checkm marker_plot {input.dir_checkm} {input.dir_asmbl} {output.dir} -x fasta --image_type svg; "
#         "checkm coding_plot {input.dir_checkm} {input.dir_asmbl} {output.dir} 0 -x fasta --image_type svg; "
#         "checkm gc_plot {input.dir_asmbl} {output.dir} 0 -x fasta --image_type svg"

# rule DRAM_annot:
#     input:
#         asmbl="../results/ONT_assembly/final_assemblies/{sample}_third_Pilon_Assembly.fasta",
#         checkm="../results/ONT_assembly/checkm_QC/checkm_QC_stats.txt",
#         config="../resources/dram_config/DRAM_config.txt"
#     output:
#         dir=directory("../results/genomes_anntations/{sample}_annot_genome")
#     log:
#         "logs/annot_QC/dram/{sample}_annotation.log"
#     threads: 10
#     # conda:
#     #     "envs/dram.yaml" TODO does not work fails at pfam step --> using cluster one
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 100000,
#         runtime= "01:00:00"
#     shell:
#         #"(DRAM-setup.py import_config --config_loc {input.config}) 2> {log}; " #useful only if using conda dram_env
#         "source /etc/profile.d/lmodstacks.sh; " #import modules for DRAM
#         "export PATH=/dcsrsoft/spack/external/dram/v1.2.4/bin:$PATH; "
#         "dcsrsoft use old; "
#         "module load gcc/9.3.0 python; "
#         "module load hmmer mmseqs2 prodigal infernal trnascan-se barrnap; "
#         "(DRAM.py annotate -i {input.asmbl} \
#          -o {output.dir} --min_contig_size 999 \
#          --threads {threads} --verbose \
#          --checkm_quality {input.checkm}) 2> {log}" #annotate genomes with DRAM

# rule DRAM_distill:
#     input:
#         dir="../results/genomes_anntations/{sample}_annot_genome"
#     output:
#         dir=directory("../results/genomes_anntations/{sample}_distill")
#     log:
#         "logs/annot_QC/dram/{sample}_distill.log"
#     threads: 10
#     # conda:
#     #     "envs/dram.yaml" TODO does not work fails at pfam step --> using cluster one
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 10000,
#         runtime= "01:00:00"
#     shell:
#         #"(DRAM-setup.py import_config --config_loc {input.config}) 2> {log}; " #useful only if using conda dram_env
#         "source /etc/profile.d/lmodstacks.sh; " #import modules for DRAM
#         "export PATH=/dcsrsoft/spack/external/dram/v1.2.4/bin:$PATH; "
#         "dcsrsoft use old; "
#         "module load gcc/9.3.0 python; "
#         "module load hmmer mmseqs2 prodigal infernal trnascan-se barrnap; "
#         "(DRAM.py distill -i {input.dir}/annotations.tsv -o {output} --trna_path {input.dir}/trnas.tsv --rrna_path {input.dir}/rrnas.tsv) 2> {log}"

# ###################################DEFENSE SYSTEMS ##########################################################################
# rule DF_launch:
#     input:
#         faa="../results/genomes_anntations/{sample}_annot_genome/genes.faa"
#     output:
#         directory("../results/AVD_output/DF_output/{sample}_DF_output")
#     conda:
#         "envs/DF_env.yaml"
#     threads: 10
#     log:
#         "logs/defense_systems/DF/{sample}_DF.log"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 10000,
#         runtime= "00:30:00"
#     shell:
#         #"defense-finder update; " # run only the first time you launch this ule, Idk how to make it more elegant
#         "(defense-finder run {input.faa} --out-dir {output}) 2> {log}"
