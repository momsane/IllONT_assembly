#wd=/work/FAC/FBM/DMF/pengel/general_data/syncom_ONT_seq/workflow/scripts
#command to symlink scratch: ln -s /scratch/mgarci14 ../../results/scratch
#workflow = scripts, envs, config
#data
#results
#logs
#benchmarks
configfile: "../config/config.yaml"
metadata_file = "../config/metadata.tsv"
# Contains:
# - one column for sample,
# - one column with 1/0 to report if all the data is available,
# - one column with the type of adapter to trim
# - one column indicating if the samples passes the quality controls

# Import packages
import numpy as np
import pandas as pd

#Read config file
metadata = pd.read_table(metadata_file, sep='\t', header=0).set_index('sample', drop = False)

#List samples to include for the read pre-processing and assembly
SAMPLES_ASS = metadata.loc[metadata['Illumina_avail'] == 1, 'sample'].tolist()
#List samples for which a genome was assembled
SAMPLES_GEN = metadata.loc[metadata['genome_assembled'] == 1, 'sample'].tolist()
#List samples to include for the downstream genome analysis
SAMPLES_ANNOT = metadata.loc[metadata['include_downstream'] == 1, 'sample'].tolist()

rule all:
    input:
        # First chunk: Illumina reads QC - add column "Illumina_adapter" to metadata at the end
        "../../results/Illumina/check_integrity/integrity_files.txt",
        "../../results/Illumina/count_reads_bt/count_reads_bt.txt",
        "../../results/Illumina/fastqc_pretrim/all",
        # Second chunk: Reads pre-processing and assembly - add column "genome_assembled" to metadata at the end
        "../../results/Illumina/fastqc_posttrim/all",
        "../../results/ONT/check_integrity/integrity_files.txt",
        expand("../../results/ONT/readstats_{stage}/readstats_{stage}_all.txt", stage = ["bf", "af"]),
        "../../results/hybracter/test",
        # expand("../../results/hybracter/{sample}_hybracter", sample = SAMPLES_ASS),
        expand("../../results/hybracter/{sample}_hybracter", sample = SAMPLES_GEN),
        # Third chunk: assemblies QC - add column "include_downstream" to metadata at the end
        "../../results/hybracter/hybracter_summary_all.tsv",
        "../../results/assembly/samples_assembly_stats.txt",
        expand("../../results/assembly/bandage/{sample}_assembly_graph.jpg", sample = SAMPLES_GEN),
        expand("../../results/final_assemblies/{sample}.fna", sample = SAMPLES_GEN),
        expand("../../results/final_assemblies_simple/{sample}.fna", sample = SAMPLES_GEN),
        "../../results/checkm",
        # Fourth chunk: polishing QC & genome analysis
        "../../results/Illumina_mapping/all_read_mapstats.txt",
        "../../results/checkm_plots",
        "../../results/gtdbtk_classify",
        expand("../../results/annotation/DRAM_mainoutputs/{sample}", sample = SAMPLES_ANNOT)

# The pipeline assumes the reads from different lanes are already
# concatenated into a single file 

# File names should be something like: {sample}_R1.fastq.gz for Illumina and {sample}.fastq.gz for ONT

############################ Reads processing ############################

# First we QC the Illumina reads

# Check that all raw read files are complete

rule gzip_test_Illumina:
    input:
        R1 = config["Illumina_raw"] + "/{sample}_R1.fastq.gz",
        R2 = config["Illumina_raw"] + "/{sample}_R2.fastq.gz",
    output:
        "../../results/Illumina/check_integrity/samples/{sample}.txt"
    log:
        "../../logs/Illumina/check_integrity/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "20m",
        mem_mb = 500
    shell:
        """
        if gzip -t {input.R1}; then
            echo {input.R1}": OK" > {output}
        else 
            echo {input.R1}": Inappropriate file type or format" > {output}
        fi
        if gzip -t {input.R2}; then
            echo {input.R2}": OK" >> {output}
        else 
            echo {input.R2}": Inappropriate file type or format" >> {output}
        fi
        """

# Incomplete/empty files are now printed to one file.
# Check the output of the following rule

rule check_integrity:
    input:
        expand("../../results/{platform}/check_integrity/samples/{sample}.txt", sample = SAMPLES_ASS, platform = "{platform}")
    output:
        "../../results/{platform}/check_integrity/integrity_files.txt"
    log:
        "../../logs/{platform}/check_integrity/all.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "10m",
        mem_mb = 100
    shell:
        """
        echo {input} | xargs cat >> {output}
        """

# Count number of reads in each fastq file before trimming.

rule count_reads_bt:
    input:
        R1 = config["Illumina_raw"] + "/{sample}_R1.fastq.gz",
        R2 = config["Illumina_raw"] + "/{sample}_R2.fastq.gz",
    output:
        "../../results/Illumina/count_reads_bt/samples/count_reads_bt_{sample}.txt"
    params:
        tmp = "../../results/Illumina/count_reads_bt/samples/count_reads_bt_{sample}.temp.txt"
    conda:
        "../envs/seqkit-2.6.1.yaml"
    log:
        "../../logs/Illumina/count_reads_bt/samples/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "1h",
        mem_mb = 2000
    shell:
        """ 
        seqkit stats -T -b -e {input.R1} > {params.tmp};
        seqkit stats -T -b -e {input.R2} >> {params.tmp};
        cat {params.tmp} | awk -v sample={wildcards.sample} 'NR == 2 {{print sample"\tR1\t"$0}}' > {output};
        cat {params.tmp} | awk -v sample={wildcards.sample} 'NR == 2 {{print sample"\tR2\t"$0}}' >> {output};
        rm {params.tmp}
        """

# Combining all the samples stats into one file 

rule combine_readcounts_bt:
    input:
        expand("../../results/Illumina/count_reads_bt/samples/count_reads_bt_{sample}.txt", sample = SAMPLES_ASS)
    output:
        "../../results/Illumina/count_reads_bt/count_reads_bt.txt"
    log:
        "../../logs/Illumina/count_reads_bt/combine_readcounts.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 1000
    shell:
        """
        echo -e "sample\tread\tfile\tformat\ttype\tnum_seqs\tsum_len\tmin_len\tavg_len\tmax_len" > {output};
        echo {input} | xargs cat >> {output}
        """

rule fastqc_bt:
    input:
        R1 = config["Illumina_raw"] + "/{sample}_R1.fastq.gz",
        R2 = config["Illumina_raw"] + "/{sample}_R2.fastq.gz",
    output:
        directory("../../results/Illumina/fastqc_pretrim/samples/{sample}")
    conda:
        "../envs/fastqc-0.11.8.yaml"
    log:
        "../../logs/Illumina/fastqc_pretrim/samples/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "1h",
        mem_mb = 1000
    shell:
        """
        mkdir -p {output};
        fastqc -o {output} {input.R1} {input.R2}
        """

# Compile and visualize all fastqc reports together

rule multiqc_fastqc_bt:
    input:
        expand("../../results/Illumina/fastqc_pretrim/samples/{sample}", sample = SAMPLES_ASS)
    output:
        directory("../../results/Illumina/fastqc_pretrim/all")
    conda:
        "../envs/multiqc-1.6.yaml"
    params:
        ignore = ".zip"
    log:
        "../../logs/Illumina/fastqc_pretrim/multiqc.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 2000
    shell:
        "multiqc --interactive -f --ignore {params.ignore} -o {output} {input}"

# The metadata file is updated to indicate the adapter content and read length
# Then you need to upload the correct adapter sequence(s) to the data folder

### Trimming

def determine_adapters(wildcards):
    adapt=metadata.loc[metadata['sample'] == wildcards.sample, 'Illumina_adapter'].item()
    if (adapt=='NT'):
        return '../../data/adapters/Nextera_Transposase.fa'
    elif (adapt=='IU'):
        return '../../data/adapters/Illumina_Universal.fa'
    elif (adapt=='TS'):
        return '../../data/adapters/TruSeq.fa'

def determine_k(wildcards):
    adapt=metadata.loc[metadata['sample'] == wildcards.sample, 'Illumina_adapter'].item()
    if (adapt=='NT'):
        return 31 #maximum allowed by bbduk
    elif (adapt=='IU'):
        return 19
    elif (adapt=='TS'):
        return 31 

# hdist is set to 2 to allow for 2 mismatches in the adapter sequence
# which more stringent than the default of 1

rule bbduk_adapt:
    input:
        R1 = config["Illumina_raw"] + "/{sample}_R1.fastq.gz",
        R2 = config["Illumina_raw"] + "/{sample}_R2.fastq.gz",
    output:
        trim1 = "../../results/Illumina/trimmed_reads/{sample}_R1.trim.fastq.gz",
        trim2 = "../../results/Illumina/trimmed_reads/{sample}_R2.trim.fastq.gz"
    params:
        adapt = determine_adapters,
        k = determine_k,
        mink = 11,
        hdist = 2,
        hdist2 = 0,
        overlap = 12,
        minlen = 40,
        q = 26
    log:
        "../../logs/Illumina/read_trimming/{sample}_trimming"
    threads: 2
    conda:
        "../envs/bbmap-39.01.yaml"
    resources:
        account = "pengel_general_data",
        runtime = "1h",
        mem_mb = 2000
    shell:
        """
        bbduk.sh in={input.R1} in2={input.R2} out={output.trim1} out2={output.trim2} \
        ref={params.adapt} ktrim=r k={params.k} mink={params.mink} \
        hdist={params.hdist} hdist2={params.hdist2} \
        tpe=t tbo=t minoverlap={params.overlap} minlen={params.minlen} rcomp=f \
        qtrim=rl trimq={params.q}
        """

# Run fastqc after trimming

rule fastqc_pt:
    input:
        R1 = "../../results/Illumina/trimmed_reads/{sample}_R1.trim.fastq.gz",
        R2 = "../../results/Illumina/trimmed_reads/{sample}_R2.trim.fastq.gz"
    output:
        directory("../../results/Illumina/fastqc_posttrim/samples/{sample}")
    conda:
        "../envs/fastqc-0.11.8.yaml"
    log:
        "../../logs/Illumina/fastqc_posttrim/samples/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "1h",
        mem_mb = 1000
    shell:
        """
        mkdir -p {output};
        fastqc -o {output} {input.R1} {input.R2}
        """

# Compile and visualize all fastqc reports together

rule multiqc_fastqc_pt:
    input:
        expand("../../results/Illumina/fastqc_posttrim/samples/{sample}", sample = SAMPLES_ASS)
    output:
        directory("../../results/Illumina/fastqc_posttrim/all")
    conda:
        "../envs/multiqc-1.6.yaml"
    params:
        ignore = ".zip"
    log:
        "../../logs/Illumina/fastqc_posttrim/multiqc.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 2000
    shell:
        "multiqc --interactive -f --ignore {params.ignore} -o {output} {input}"

############################ ONT reads processing ############################

# File names should be something like: {sample}.fastq.gz

# Check that files are intact
# Not needed if you could verify the md5sums

rule gzip_test_ONT:
    input:
        config["ONT_raw"] + "/{sample}.fastq.gz"
    output:
        "../../results/ONT/check_integrity/samples/{sample}.txt"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "20m",
        mem_mb = 500
    shell:
        """
        if gzip -t {input}; then
            echo {input}": OK" > {output}
        else 
            echo {input}": Inappropriate file type or format" > {output}
        fi
        """

# Reuse rule check_integrity

### QC and trimming/filtering

# First we want to compute a few stats on the raw reads

rule readstats_bf:
    input:
        config["ONT_raw"] + "/{sample}.fastq.gz"
    output:
        "../../results/ONT/readstats_bf/samples/readstats_bf_{sample}.txt"
    conda:
        "../envs/nanoq-0.10.0.yaml"
    log:
        "../../logs/ONT/readstats_bf/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "1h",
        mem_mb = 2000
    shell:
        """ 
        nanoq -i {input} --json -s -vv > {output}
        """

# Combining all the read stats into one file: this rule parses all json outputs to turn
# them into a tabular format then combines them into one file

rule combine_readstats:
    input:
        expand("../../results/ONT/readstats_{stage}/samples/readstats_{stage}_{sample}.txt", sample = SAMPLES_ASS, stage = "{stage}")
    output:
        "../../results/ONT/readstats_{stage}/readstats_{stage}_all.txt"
    log:
        "../../logs/ONT/readstats_{stage}/all.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 1000
    run:
        import json
        import pandas as pd
        
        combined_df = pd.DataFrame()
        
        for input_file in input:
            sample_name = input_file.split('_')[-1].replace('.txt', '')
            
            with open(input_file, 'r') as f:
                data = json.load(f)
                data['sample'] = sample_name  # Add sample to the data
                df = pd.DataFrame.from_dict(pd.json_normalize(data), orient='columns')
                
                combined_df = pd.concat([combined_df, df], ignore_index=True)
        
        combined_df.set_index('sample', inplace=True)
        combined_df.to_csv(output[0], sep='\t', index=True)

# Then we filter the reads to remove short and low quality reads

rule read_filtering:
    input:
        config["ONT_raw"] + "/{sample}.fastq.gz"
    output:
        "../../results/ONT/filtered_reads/{sample}_ONT_filtered.fastq.gz"
    params:
        min_readlen = 800,
        min_meanq = 10,
        length_weight = 10,
        target_bases = 400000000
    conda:
        "../envs/filtlong-0.2.1.yaml"
    log:
        "../../logs/ONT/filtlong/{sample}_filtlong.log"
    benchmark:
        "../../benchmarks/filtlong/{sample}_filtlong.benchmark"
    threads: 2
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 2000
    shell:
        """
        (filtlong --min_length {params.min_readlen} \
        --min_mean_q {params.min_meanq} \
        --length_weight {params.length_weight} \
        --target_bases {params.target_bases}  \
        {input} | gzip > {output})2> {log}
        """

# Recompute read stats after filtering

rule readstats_af:
    input:
        "../../results/ONT/filtered_reads/{sample}_ONT_filtered.fastq.gz"
    output:
        "../../results/ONT/readstats_af/samples/readstats_af_{sample}.txt"
    conda:
        "../envs/nanoq-0.10.0.yaml"
    log:
        "../../logs/ONT/readstats_af/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "1h",
        mem_mb = 2000
    shell:
        """ 
        nanoq -i {input} --json -s -vv > {output}
        """

# Combine all the read stats into one file: reuse rule combine_readstats

############################ Hybracter ############################

# Hybracter is a snakemake-based software that runs the flye assembly,
# polishing with long reads (medaka, two rounds)
# polishing with short reads (polypolish and pypolca, 1 round each)
# with intermediate quality controls and comparisons between each polishing step

rule init_hybracter:
    input:
        expand("../../results/ONT/filtered_reads/{sample}_ONT_filtered.fastq.gz", sample = SAMPLES_ASS)
    output:
        install = directory("../../results/hybracter/installation"),
        test = directory("../../results/hybracter/test")
    params:
        config = "../config/config_hybracter.yaml"
    log:
        "../../logs/hybracter/init.log"
    threads: 1
    conda:
        "../envs/hybracter-0.9.0.yaml"
    resources:
        account = "pengel_general_data",
        mem_mb = 8000,
        runtime= "30m"
    shell:
        """
        hybracter install --configfile {params.config} -o {output.install};
        hybracter test-hybrid --configfile {params.config} -o {output.test}
        """

def get_size(wildcards):
    size = metadata.loc[metadata['sample'] == wildcards.sample, 'estimated_genome_size'].item()
    return int(size)

rule hybracter:
    input:
        install = "../../results/hybracter/installation",
        test = "../../results/hybracter/test",
        filtered_ont = "../../results/ONT/filtered_reads/{sample}_ONT_filtered.fastq.gz",
        R1 = "../../results/Illumina/trimmed_reads/{sample}_R1.trim.fastq.gz",
        R2 = "../../results/Illumina/trimmed_reads/{sample}_R2.trim.fastq.gz"
    output:
        fld = directory("../../results/hybracter/{sample}_hybracter"),
        summary = "../../results/hybracter/{sample}_hybracter/FINAL_OUTPUT/hybracter_summary.tsv"
    params:
        size = get_size,
        config = "../config/config_hybracter.yaml"
    benchmark:
        "../../benchmarks/hybracter/{sample}_hybracter.benchmark"
    log:
        "../../logs/hybracter/{sample}_hybracter.log"
    conda:
        "../envs/hybracter-0.9.0.yaml"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 8000,
        runtime= "5h"
    shell:
        """
        hybracter hybrid-single --configfile {params.config} -l {input.filtered_ont} \
        -1 {input.R1} -2 {input.R2} \
        -s {wildcards.sample} -c {params.size} -o {output} \
        -t {threads} --skip_qc --logic best    
        """

############################ Genomes QC ############################

# If some samples are of poor quality, hybracter will exit without producing an output
# so we need to update the /config/metadata.tsv file to exclude them from downstream analysis

# First we combine the hybracter summary outputs

rule hybracter_summary:
    input:
        fld = expand("../../results/hybracter/{sample}_hybracter", sample = SAMPLES_GEN),
        summaries = expand("../../results/hybracter/{sample}_hybracter/FINAL_OUTPUT/hybracter_summary.tsv", sample = SAMPLES_GEN)
    output:
        "../../results/hybracter/hybracter_summary_all.tsv"
    log:
        "../../logs/hybracter/summary_all.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 500,
        runtime= "5m"
    shell:
        """
        echo -e 'Sample\tComplete\tTotal_assembly_length\tNumber_of_contigs\tMost_accurate_polishing_round\tLongest_contig_length\tLongest_contig_coverage\tNumber_circular_plasmids' > {output};
        echo {input.summaries} | xargs tail -q -n +2 >> {output}
        """

# Then we combine the assembly stats from flye

rule flye_assembly_stats:
    input:
        fld = expand("../../results/hybracter/{sample}_hybracter", sample = SAMPLES_GEN),
        stats = expand("../../results/hybracter/{sample}_hybracter/supplementary_results/flye_individual_summaries/{sample}_assembly_info.txt", sample = SAMPLES_GEN)
    output:
        "../../results/assembly/samples_assembly_stats.txt"
    log:
        "../../logs/assembly/assembly_stats.log"
    params:
        tmp = "../../results/assembly/tmp.txt"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 500,
        runtime= "10m"
    shell:
        """
        echo -e 'file\tseq_name\tlength\tcoverage\tcircular\trepeat\tmult\ttelomere\talt_group\tgraph_path' > {params.tmp};
        for file in {input.stats}; do
        awk -v fname="$file" -F '\t' 'NR > 1 {{print fname"\t"$0}}' "$file" >> {params.tmp};
        done;
        sed '/#/ d' < {params.tmp} > {output};
        rm {params.tmp}
        """

# And visualize the assembly graph

rule bandage_assembly:
    input:
        fld = "../../results/hybracter/{sample}_hybracter"
    output:
        "../../results/assembly/bandage/{sample}_assembly_graph.jpg"
    params:
        graph = "../../results/hybracter/{sample}_hybracter/processing/assemblies/{sample}/assembly_graph.gfa"
    log:
        "../../logs/assembly/{sample}_assembly_graph.log"
    conda:
        "../envs/bandage-0.8.1.yaml"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 2000,
        runtime= "2h"
    shell:
        "Bandage image {params.graph} {output}"

# After this check the hybracter output and genome QC outputs and indicate in the metadata file
# whether the assembly seems complete or not

# Rename contigs to have sample name, replace all spaces and "=" by "_" (it is crucial to not have any "=" for DRAM)
# and put all complete assemblies in a single folder

def get_fasta(wildcards):
    if metadata.loc[metadata['sample'] == wildcards.sample, 'complete'].item() == 1:
        return "../../results/hybracter/" + wildcards.sample + "_hybracter/FINAL_OUTPUT/complete/" + wildcards.sample + "_final.fasta"
    else:
        return "../../results/hybracter/" + wildcards.sample + "_hybracter/FINAL_OUTPUT/incomplete/" + wildcards.sample + "_final.fasta"

rule final_assemblies:
    input:
        fld = "../../results/hybracter/{sample}_hybracter"
    output:
        "../../results/final_assemblies/{sample}.fna"
    params:
        fld = "../../results/final_assemblies",
        fasta = get_fasta
    log:
        "../../logs/final_assemblies/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 1000,
        runtime= "10m"
    shell:
        """
        mkdir -p {params.fld};
        sample={wildcards.sample};
        sed -e "s/>/>$sample /g" -e "s/=/_/g" -e "s/ /_/g" {params.fasta} > {output}
        """

# A second version of the assembly with simplified fasta headers and 
# without the sample name because DRAM will add it itself

rule final_assemblies_nosample:
    input:
        fld = "../../results/hybracter/{sample}_hybracter",
        awk_script = "rename_scaffolds.awk",
    output:
        tmp = temp("../../results/final_assemblies_simple/{sample}.tmp"),
        fna = "../../results/final_assemblies_simple/{sample}.fna"
    params:
        fld = "../../results/final_assemblies_simple",
        fasta = get_fasta
    log:
        "../../logs/final_assemblies_simple/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 1000,
        runtime= "10m"
    shell:
        """
        mkdir -p {params.fld};
        sed -e "s/=/_/g" -e "s/ /_/g" {params.fasta} > {output.tmp};
        awk -f {input.awk_script} {output.tmp} > {output.fna}
        """

# Next we run checkM to assess the quality of the assemblies

rule checkm:
    input:
        expand("../../results/final_assemblies/{sample}.fna", sample = SAMPLES_GEN)
    output:
        fld = directory("../../results/checkm"),
        rextended = "../../results/checkm/checkm_report_extended.txt"
    params:
        fld = "../../results/final_assemblies",
        extension = "fna",
        tmpdir = "../../results/scratch/checkm.tmp",
        tmprep = "../../results/checkm/checkm_report_simple.txt",
        lineage = "../../results/checkm/lineage.ms"
    log:
        "../../logs/checkm/checkm.log"
    conda:
        "../envs/checkm-1.2.2.yaml"
    threads: 2
    resources:
        account = "pengel_general_data",
        mem_mb = 100000,
        runtime = "1h"
    shell:
        """
        mkdir -p {params.tmpdir};
        checkm lineage_wf -f {params.tmprep} --tab_table -x {params.extension} \
        -t {threads} --tmpdir {params.tmpdir} {params.fld} {output.fld};
        checkm qa -o 2 -f {output.rextended} --tab_table --tmpdir {params.tmpdir} \
        -t {threads} {params.lineage} {output.fld};
        rm {params.tmprep};
        rm -rf {params.tmpdir}
        """

# Check the checkM output to see if the genomes seem complete
# From here only complete genomes will be further processed

############################ Short reads mapping coverage ############################

# Map Illumina reads to assembly (hybracter does not provide the bam file)

rule map_Illumina:
    input:
        R1 = "../../results/Illumina/trimmed_reads/{sample}_R1.trim.fastq.gz",
        R2 = "../../results/Illumina/trimmed_reads/{sample}_R2.trim.fastq.gz",
        assembly = "../../results/final_assemblies/{sample}.fna"
    output:
        bam = "../../results/Illumina_mapping/bams/{sample}_assembly_sorted.bam",
        reads = "../../results/Illumina_mapping/read_mapping/{sample}_read_mapping.txt",
        bases = "../../results/Illumina_mapping/per_base/{sample}_read_mapping_per_base.txt"
    log:
        "../../logs/Illumina_mapping/{sample}_bowtie2.log"
    benchmark:
        "../../benchmarks/Illumina_mapping/{sample}_bowtie2.benchmark"
    params:
        buildf = "../../results/Illumina_mapping/build/{sample}",
        build = "../../results/Illumina_mapping/build/{sample}/{sample}"
    conda:
        "../envs/Illumina_mapping.yaml"
    threads: 6
    resources:
        account = "pengel_general_data",
        mem_mb = 3000,
        runtime = "20m"
    shell:
        """
        mkdir -p {params.buildf};
        bowtie2-build --quiet {input.assembly} {params.build};
        bowtie2 -1 {input.R1} -2 {input.R2} -x {params.build} --threads {threads} \
        --local --very-sensitive-local | samtools sort -O BAM -o {output.bam};
        samtools index {output.bam};
        (samtools flagstat {output.bam} > {output.reads}) 2> {log};
        (samtools depth -a {output.bam} > {output.bases}) 2> {log}
        """

rule parse_ONT_assembly_mapping:
    input:
        reads = expand("../../results/Illumina_mapping/read_mapping/{sample}_read_mapping.txt", sample = SAMPLES_ANNOT),
        bases = expand("../../results/Illumina_mapping/per_base/{sample}_read_mapping_per_base.txt", sample = SAMPLES_ANNOT)
    output:
        all_reads = "../../results/Illumina_mapping/all_read_mapstats.txt",
        all_bases = "../../results/Illumina_mapping/all_bases_cov.txt"
    log:
        "../../logs/Illumina_mapping/summary_mapping.log"
    conda:
        "../envs/R-4.3.2.yaml"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 6000,
        runtime= "30m"
    script:
        "./parse_ONTassembly.R"

rule checkm_plots:
    input:
        assemblies = expand("../../results/assembly/spades/contigs_filtered/{sample}.fna", sample = SAMPLES_ANNOT),
        dir_asmbl = "../../results/final_assemblies",
        dir_checkm = "../../results/checkm"
    output:
        fld = directory("../../results/checkm_plots")
    params:
        extension = "fna"
    benchmark:
        "../../benchmarks/checkm_plots.benchmark"
    log:
        "../../logs/checkm/plot_checkm.log"
    threads: 1
    conda:
        "../envs/checkm-1.2.2.yaml"
    resources:
        account = "pengel_general_data",
        mem_mb = 2000,
        runtime= "20m"
    shell:
        "checkm marker_plot {input.dir_checkm} {input.dir_asmbl} {output.fld} -x {params.extension} --image_type svg; "
        "checkm coding_plot {input.dir_checkm} {input.dir_asmbl} {output.fld} 0 -x {params.extension} --image_type svg; "
        "checkm gc_plot {input.dir_asmbl} {output.fld} 0 -x {params.extension} --image_type svg"

############################ Classification & annotation ##########################################

rule gtdb_classify:
    input:
        "../../results/final_assemblies"
    output:
        fld = directory("../../results/gtdbtk_classify"),
        report = "../../results/gtdbtk_classify/classify/gtdbtk.bac120.summary.tsv"
    log:
        "../../logs/gtdbtk/gtdbtk_classify"
    benchmark:
        "../../benchmarks/gtdbtk_classify.benchmark"
    params:
        extension = "fna",
        mashdb = config["Mash_db"],
        tmpdir = "../../results/scratch/gtdbtk_classify/",
        scratchdir = "../../results/scratch/gtdbtk_classify_pplacer"
    conda:
        "../envs/gtdbtk-2.4.0.yaml"
    threads: 4
    resources:
        account = "pengel_general_data",
        mem_mb = 20000,
        runtime = "1h"
    shell:
        """
        mkdir -p {params.tmpdir};
        gtdbtk classify_wf --genome_dir {input} --mash_db {params.mashdb} --scratch_dir {params.scratchdir} \
        --out_dir {output.fld} --extension {params.extension} --cpus {threads} --tmpdir {params.tmpdir}
        """

rule DRAM_annot:
    input:
        asmbl = "../../results/final_assemblies_simple/{sample}.fna",
        checkm = "../../results/checkm/checkm_report_extended.txt",
        gtdbtk = "../../results/gtdbtk_classify/classify/gtdbtk.bac120.summary.tsv",
        config = "../config/dram-1.4.6_config.txt"
    output:
        directory("../../results/annotation/DRAM_annotate/{sample}_DRAM")
    log:
        "../../logs/annotation/dram/{sample}_dram.log"
    benchmark:
        "../../benchmarks/annotation/{sample}_dram.benchmark"
    conda:
        "../envs/dram-1.4.6.yaml"
    threads: 8
    resources:
        account = "pengel_general_data",
        mem_mb = 100000,
        runtime= "2h"
    shell:
        """
        (DRAM-setup.py import_config --config_loc {input.config}) 2> {log};
        (DRAM.py annotate -i {input.asmbl} \
         -o {output} --min_contig_size 999 \
         --threads {threads} --verbose \
         --checkm_quality {input.checkm} --gtdb_taxonomy {input.gtdbtk}) 2> {log} 
         """

rule DRAM_distill:
    input:
        fld = "../../results/annotation/DRAM_annotate/{sample}_DRAM",
        config = "../config/dram-1.4.6_config.txt"
    output:
        directory("../../results/annotation/DRAM_distill/{sample}_DRAM_distill")
    log:
        "../../logs/annotation/distill/{sample}_distill.log"
    benchmark:
        "../../benchmarks/annotation/{sample}_dram_distill.benchmark"
    threads: 8
    conda:
        "../envs/dram-1.4.6.yaml"
    resources:
        account = "pengel_general_data",
        mem_mb = 10000,
        runtime= "2h"
    shell:
        """
        (DRAM-setup.py import_config --config_loc {input.config}) 2> {log}; 
        (DRAM.py distill -i {input.fld}/annotations.tsv -o {output} --trna_path {input.fld}/trnas.tsv \
        --rrna_path {input.fld}/rrnas.tsv) 2> {log}
        """

# rename and reorganize the DRAM outputs

rule move_DRAM_outputs:
    input:
        dram = "../../results/annotation/DRAM_annotate/{sample}_DRAM",
        distill = "../../results/annotation/DRAM_distill/{sample}_DRAM_distill"
    output:
        directory("../../results/annotation/DRAM_mainoutputs/{sample}")
    log:
        "../../logs/annotation/move/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 1000,
        runtime= "10m"
    shell:
        """
        mkdir -p {output};
        cut -f2- {input.dram}/annotations.tsv > {output}/{wildcards.sample}_annotations.tsv;
        mv {input.dram}/genes.faa {output}/{wildcards.sample}_genes.faa;
        mv {input.dram}/genes.fna {output}/{wildcards.sample}_genes.fna;
        mv {input.dram}/rrnas.tsv {output}/{wildcards.sample}_genes.tsv;
        mv {input.dram}/trnas.tsv {output}/{wildcards.sample}_trnas.tsv;
        mv {input.dram}/genbank/{wildcards.sample}.gbk {output};
        mv {input.distill}/genome_stats.tsv {output}/{wildcards.sample}_genome_stats.tsv;
        mv {input.distill}/metabolism_summary.xlsx {output}/{wildcards.sample}_metabolism_summary.xlsx;
        mv {input.distill}/product.html {output}/{wildcards.sample}_product.html
        mv {input.distill}/product.tsv {output}/{wildcards.sample}_product.tsv
        """

# TODO
# - Defense finder
# - RGI

# ################################### DEFENSE SYSTEMS ##########################################################################
# rule DF_launch:
#     input:
#         faa="../results/genomes_anntations/{sample}_annot_genome/genes.faa"
#     output:
#         directory("../results/AVD_output/DF_output/{sample}_DF_output")
#     conda:
#         "envs/DF_env.yaml"
#     threads: 10
#     log:
#         "logs/defense_systems/DF/{sample}_DF.log"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 10000,
#         runtime= "00:30:00"
#     shell:
#         #"defense-finder update; " # run only the first time you launch this ule, Idk how to make it more elegant
#         "(defense-finder run {input.faa} --out-dir {output}) 2> {log}"
